{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, Input\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Enhanced data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Define the student model\n",
    "def create_student_model(conv1_filters, conv2_filters, conv3_filters, conv4_filters, conv5_filters, conv_filters, dense_units, dropout_rate):\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(conv1_filters, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv2_filters, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv3_filters, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(conv4_filters, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv5_filters, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv6_filters, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def distillation_loss(y_true, y_pred, teacher_logits, temperature=3, alpha=0.1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    teacher_logits = tf.cast(teacher_logits, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Compute soft targets\n",
    "    soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "    soft_prob = tf.nn.softmax(y_pred / temperature)\n",
    "    \n",
    "    # Soft targets loss\n",
    "    soft_targets_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(soft_targets, soft_prob)\n",
    "    )\n",
    "    \n",
    "    # Hard targets loss\n",
    "    student_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    )\n",
    "    \n",
    "    # Combined loss\n",
    "    return alpha * student_loss + (1 - alpha) * soft_targets_loss\n",
    "\n",
    "# Define the hyperparameter tuning function\n",
    "def build_model(hp):\n",
    "    conv1_filters = hp.Int('conv1_filters', 64, 64)\n",
    "    conv2_filters = hp.Int('conv2_filters', 192, 192)\n",
    "    conv3_filters = hp.Int('conv3_filters', 384, 384)\n",
    "    dense_units = hp.Int('dense_units', 768, 768)\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.3, 0.6, step=0.1)\n",
    "    student_model = create_student_model(conv1_filters, conv2_filters, conv3_filters, dense_units, dropout_rate)\n",
    "    optimizer = optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    student_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return student_model\n",
    "\n",
    "# Optimization for hyperparameter tuning\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    num_initial_points=3,\n",
    "    max_trials=6,  # Set to 1 trial\n",
    "    directory='kt_dir',\n",
    "    project_name='student_model_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(\n",
    "    datagen.flow(x_train, y_train, batch_size=64),\n",
    "    epochs=10,  # 20 epochs for each trial\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model\n",
    "best_student_model = create_student_model(\n",
    "    best_hps.get('conv1_filters'),\n",
    "    best_hps.get('conv2_filters'),\n",
    "    best_hps.get('conv3_filters'),\n",
    "    best_hps.get('dense_units'),\n",
    "    best_hps.get('dropout_rate')\n",
    ")\n",
    "best_student_model.compile(\n",
    "    optimizer=optimizers.Adam(best_hps.get('learning_rate')),\n",
    "    loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_model(best_student_model.input)),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for student model training\n",
    "checkpoint_callback_student = callbacks.ModelCheckpoint(\n",
    "    'student_model.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping_student = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "lr_scheduler_student = callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "\n",
    "# Train the student model with knowledge distillation\n",
    "best_student_model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=128),\n",
    "    epochs=50,  # Train the final model for more epochs\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[checkpoint_callback_student, early_stopping_student, lr_scheduler_student]\n",
    ")\n",
    "\n",
    "# Evaluate the student model\n",
    "eval_results = best_student_model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {eval_results[0]}')\n",
    "print(f'Test accuracy: {eval_results[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models, optimizers, callbacks\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/__init__.py:138\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    122\u001b[0m     concat,\n\u001b[1;32m    123\u001b[0m     lreshape,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     qcut,\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/api/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     extensions,\n\u001b[1;32m      4\u001b[0m     indexers,\n\u001b[1;32m      5\u001b[0m     interchange,\n\u001b[1;32m      6\u001b[0m     types,\n\u001b[1;32m      7\u001b[0m     typing,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/api/typing/__init__.py:31\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Expanding,\n\u001b[1;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     Window,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[1;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/io/json/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     read_json,\n\u001b[1;32m      3\u001b[0m     to_json,\n\u001b[1;32m      4\u001b[0m     ujson_dumps,\n\u001b[1;32m      5\u001b[0m     ujson_loads,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/io/json/_json.py:71\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     68\u001b[0m     build_table_schema,\n\u001b[1;32m     69\u001b[0m     parse_table_schema,\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m         Hashable,\n\u001b[1;32m     76\u001b[0m         Mapping,\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/io/parsers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     TextFileReader,\n\u001b[1;32m      3\u001b[0m     TextParser,\n\u001b[1;32m      4\u001b[0m     read_csv,\n\u001b[1;32m      5\u001b[0m     read_fwf,\n\u001b[1;32m      6\u001b[0m     read_table,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/jupyter venv/venv/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     34\u001b[0m     AbstractMethodError,\n\u001b[1;32m     35\u001b[0m     ParserWarning,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "File \u001b[0;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Enhanced data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "def create_student_model(conv1_filters, conv2_filters, conv3_filters, conv4_filters, conv5_filters, conv6_filters, dense_units, dropout_rate):\n",
    "    he_init = HeNormal()\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(conv1_filters, (3, 3), activation='relu', kernel_initializer=he_init)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv2_filters, (3, 3), activation='relu', kernel_initializer=he_init)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv3_filters, (3, 3), activation='relu', kernel_initializer=he_init)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(conv4_filters, (3, 3), activation='relu', kernel_initializer=he_init)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv5_filters, (3, 3), activation='relu', kernel_initializer=he_init)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv6_filters, (3, 3), activation='relu', kernel_initializer=he_init)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu', kernel_initializer=he_init)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax', kernel_initializer=he_init)(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def distillation_loss(y_true, y_pred, teacher_logits, temperature=3, alpha=0.1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    teacher_logits = tf.cast(teacher_logits, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Compute soft targets\n",
    "    soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "    soft_prob = tf.nn.softmax(y_pred / temperature)\n",
    "    \n",
    "    # Soft targets loss\n",
    "    soft_targets_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(soft_targets, soft_prob)\n",
    "    )\n",
    "    \n",
    "    # Hard targets loss\n",
    "    student_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    )\n",
    "    \n",
    "    # Combined loss\n",
    "    return alpha * student_loss + (1 - alpha) * soft_targets_loss\n",
    "\n",
    "# Define the hyperparameter tuning function\n",
    "def build_model(hp):\n",
    "    conv1_filters = hp.Int('conv1_filters', 64, 64)\n",
    "    conv2_filters = hp.Int('conv2_filters', 192, 192)\n",
    "    conv3_filters = hp.Int('conv3_filters', 384, 384)\n",
    "    dense_units = hp.Int('dense_units', 768, 768)\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.3, 0.6, step=0.1)\n",
    "    student_model = create_student_model(conv1_filters, conv2_filters, conv3_filters, dense_units, dropout_rate)\n",
    "    optimizer = optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    student_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return student_model\n",
    "\n",
    "# Optimization for hyperparameter tuning\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    num_initial_points=3,\n",
    "    max_trials=6,  # Set to 1 trial\n",
    "    directory='kt_dir',\n",
    "    project_name='student_model_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(\n",
    "    datagen.flow(x_train, y_train, batch_size=64),\n",
    "    epochs=10,  # 20 epochs for each trial\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model\n",
    "best_student_model = create_student_model(\n",
    "    best_hps.get('conv1_filters'),\n",
    "    best_hps.get('conv2_filters'),\n",
    "    best_hps.get('conv3_filters'),\n",
    "    best_hps.get('dense_units'),\n",
    "    best_hps.get('dropout_rate')\n",
    ")\n",
    "best_student_model.compile(\n",
    "    optimizer=optimizers.Adam(best_hps.get('learning_rate')),\n",
    "    loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_model(best_student_model.input)),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for student model training\n",
    "checkpoint_callback_student = callbacks.ModelCheckpoint(\n",
    "    'student_model.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping_student = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "lr_scheduler_student = callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "\n",
    "# Train the student model with knowledge distillation\n",
    "best_student_model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=128),\n",
    "    epochs=50,  # Train the final model for more epochs\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[checkpoint_callback_student, early_stopping_student, lr_scheduler_student]\n",
    ")\n",
    "\n",
    "# Evaluate the student model\n",
    "eval_results = best_student_model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {eval_results[0]}')\n",
    "print(f'Test accuracy: {eval_results[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
