{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Enhanced data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "# Define the teacher model\n",
    "def create_teacher_model():\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(1028, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    return models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "teacher_model = create_teacher_model()\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "teacher_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Callbacks for teacher model\n",
    "checkpoint_callback_teacher = callbacks.ModelCheckpoint(\n",
    "    'teacher_model.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping_teacher = callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "lr_scheduler_teacher = callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# Train the teacher model\n",
    "teacher_model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=256),\n",
    "    epochs=50,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[checkpoint_callback_teacher, early_stopping_teacher, lr_scheduler_teacher]\n",
    ")\n",
    "\n",
    "# Evaluate the teacher model\n",
    "eval_results_teacher = teacher_model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {eval_results_teacher[0]}')\n",
    "print(f'Test accuracy: {eval_results_teacher[1]}')\n",
    "\n",
    "# Define the student model\n",
    "def create_student_model(conv1_filters, conv2_filters, conv3_filters, dense_units, dropout_rate):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(conv1_filters, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(conv2_filters, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(conv3_filters, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(dense_units, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def distillation_loss(y_true, y_pred, teacher_logits, temperature=3, alpha=0.1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    teacher_logits = tf.cast(teacher_logits, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Compute soft targets\n",
    "    soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "    soft_prob = tf.nn.softmax(y_pred / temperature)\n",
    "    \n",
    "    # Soft targets loss\n",
    "    soft_targets_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(soft_targets, soft_prob)\n",
    "    )\n",
    "    \n",
    "    # Hard targets loss\n",
    "    student_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    )\n",
    "    \n",
    "    # Combined loss\n",
    "    return alpha * student_loss + (1 - alpha) * soft_targets_loss\n",
    "\n",
    "# Define the hyperparameter tuning function\n",
    "def build_model(hp):\n",
    "    conv1_filters = hp.Int('conv1_filters', 32, 128, step=32)\n",
    "    conv2_filters = hp.Int('conv2_filters', 64, 256, step=64)\n",
    "    conv3_filters = hp.Int('conv3_filters', 128, 512, step=128)\n",
    "    dense_units = hp.Int('dense_units', 128, 512, step=128)\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.3, 0.6, step=0.1)\n",
    "    student_model = create_student_model(conv1_filters, conv2_filters, conv3_filters, dense_units, dropout_rate)\n",
    "    optimizer = optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    student_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return student_model\n",
    "\n",
    "# Optimization for hyperparameter tuning\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    num_initial_points=1,\n",
    "    max_trials=1,  # Set to 1 trial\n",
    "    directory='kt_dir',\n",
    "    project_name='student_model_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(\n",
    "    datagen.flow(x_train, y_train, batch_size=128),\n",
    "    epochs=50,  # 20 epochs for each trial\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model\n",
    "best_student_model = create_student_model(\n",
    "    best_hps.get('conv1_filters'),\n",
    "    best_hps.get('conv2_filters'),\n",
    "    best_hps.get('conv3_filters'),\n",
    "    best_hps.get('dense_units'),\n",
    "    best_hps.get('dropout_rate')\n",
    ")\n",
    "best_student_model.compile(\n",
    "    optimizer=optimizers.Adam(best_hps.get('learning_rate')),\n",
    "    loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_model(best_student_model.input)),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for student model training\n",
    "checkpoint_callback_student = callbacks.ModelCheckpoint(\n",
    "    'student_model.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping_student = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "lr_scheduler_student = callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "\n",
    "# Train the student model with knowledge distillation\n",
    "best_student_model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=128),\n",
    "    epochs=50,  # Train the final model for more epochs\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[checkpoint_callback_student, early_stopping_student, lr_scheduler_student]\n",
    ")\n",
    "\n",
    "# Evaluate the student model\n",
    "eval_results = best_student_model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {eval_results[0]}')\n",
    "print(f'Test accuracy: {eval_results[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def continuous_train(model, datagen, x_train, y_train, x_test, y_test, model_name, epochs=50, batch_size=128):\n",
    "    # Check if there are existing weights to load\n",
    "    weights_path = f'{model_name}.weights.h5'\n",
    "    if os.path.exists(weights_path):\n",
    "        print(f'Loading existing weights from {weights_path}')\n",
    "        model.load_weights(weights_path)\n",
    "    else:\n",
    "        print(f'No existing weights found. Training from scratch.')\n",
    "\n",
    "    # Callbacks for training\n",
    "    checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        weights_path,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    lr_scheduler = callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_scheduler]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = model.evaluate(x_test, y_test)\n",
    "    print(f'Test loss: {eval_results[0]}')\n",
    "    print(f'Test accuracy: {eval_results[1]}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def continuous_train_with_distillation(student_model, teacher_model, datagen, x_train, y_train, x_test, y_test, model_name, epochs=50, batch_size=128, temperature=3, alpha=0.1):\n",
    "    weights_path = f'{model_name}.weights.h5'\n",
    "    if os.path.exists(weights_path):\n",
    "        print(f'Loading existing weights from {weights_path}')\n",
    "        student_model.load_weights(weights_path)\n",
    "    else:\n",
    "        print(f'No existing weights found. Training from scratch.')\n",
    "\n",
    "    # Define distillation loss\n",
    "    def distillation_loss(y_true, y_pred, teacher_logits):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        teacher_logits = tf.cast(teacher_logits, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        \n",
    "        # Compute soft targets\n",
    "        soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "        soft_prob = tf.nn.softmax(y_pred / temperature)\n",
    "        \n",
    "        # Soft targets loss\n",
    "        soft_targets_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.categorical_crossentropy(soft_targets, soft_prob)\n",
    "        )\n",
    "        \n",
    "        # Hard targets loss\n",
    "        student_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        return alpha * student_loss + (1 - alpha) * soft_targets_loss\n",
    "\n",
    "    # Compile student model with distillation loss\n",
    "    student_model.compile(\n",
    "        optimizer=optimizers.Adam(),\n",
    "        loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_model(student_model.input)),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Callbacks for training\n",
    "    checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "        weights_path,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    lr_scheduler = callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "\n",
    "    # Train the student model\n",
    "    student_model.fit(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[checkpoint_callback, early_stopping, lr_scheduler]\n",
    "    )\n",
    "\n",
    "    # Evaluate the student model\n",
    "    eval_results = student_model.evaluate(x_test, y_test)\n",
    "    print(f'Test loss: {eval_results[0]}')\n",
    "    print(f'Test accuracy: {eval_results[1]}')\n",
    "    \n",
    "    return student_model\n",
    "\n",
    "# Example usage for the teacher model:\n",
    "best_teacher_model = continuous_train(\n",
    "    best_teacher_model,\n",
    "    datagen,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    model_name='teacher_model',\n",
    "    epochs=50,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# Example usage for the student model:\n",
    "best_student_model = continuous_train(\n",
    "    best_student_model,\n",
    "    datagen,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    model_name='student_model',\n",
    "    epochs=50,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "\n",
    "# Example usage for the student model with knowledge distillation:\n",
    "best_student_model = continuous_train_with_distillation(\n",
    "    best_student_model,\n",
    "    best_teacher_model,\n",
    "    datagen,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    model_name='student_model',\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    temperature=3,\n",
    "    alpha=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
