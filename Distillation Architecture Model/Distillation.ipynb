{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks, Input\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding for the teacher model\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Teacher Model\n",
    "def create_teacher_model():\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "teacher_model = create_teacher_model()\n",
    "teacher_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.00027679),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train Teacher Model\n",
    "teacher_history = teacher_model.fit(\n",
    "    datagen.flow(x_train, y_train_one_hot, batch_size=128),\n",
    "    epochs=50,\n",
    "    validation_data=(x_test, y_test_one_hot),\n",
    "    callbacks=[callbacks.ModelCheckpoint('teacher_model_best.h5.keras', save_best_only=True)]\n",
    ")\n",
    "\n",
    "# Save the final teacher model\n",
    "teacher_model.save('teacher_model_final.h5.keras')\n",
    "\n",
    "# Student Model\n",
    "def create_student_model(conv1_filters, conv2_filters, conv3_filters, dense_units, dropout_rate):\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(conv1_filters, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv2_filters, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(conv3_filters, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "student_model = create_student_model(\n",
    "    conv1_filters=64,\n",
    "    conv2_filters=192,\n",
    "    conv3_filters=384,\n",
    "    dense_units=768,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "student_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.00035361),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train Student Model\n",
    "student_history = student_model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=64),\n",
    "    epochs=40,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[\n",
    "        callbacks.ModelCheckpoint('student_model_best.h5.keras', save_best_only=True),\n",
    "        callbacks.EarlyStopping(patience=5),\n",
    "        callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save the final student model\n",
    "student_model.save('student_model_final.h5.keras')\n",
    "\n",
    "# Distillation\n",
    "def distillation_loss(y_true, y_pred, teacher_logits, temperature=3, alpha=0.1):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    teacher_logits = tf.cast(teacher_logits, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
    "    soft_prob = tf.nn.softmax(y_pred / temperature)\n",
    "    \n",
    "    soft_targets_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.categorical_crossentropy(soft_targets, soft_prob)\n",
    "    )\n",
    "    \n",
    "    student_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    )\n",
    "    \n",
    "    return alpha * student_loss + (1 - alpha) * soft_targets_loss\n",
    "\n",
    "# Load best teacher model for distillation\n",
    "best_teacher_model = tf.keras.models.load_model('teacher_model_best.h5.keras')\n",
    "\n",
    "# Create a new student model for distillation\n",
    "distilled_student_model = create_student_model(\n",
    "    conv1_filters=64,\n",
    "    conv2_filters=192,\n",
    "    conv3_filters=384,\n",
    "    dense_units=768,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "# Custom training step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        teacher_logits = best_teacher_model(x, training=False)\n",
    "        student_logits = distilled_student_model(x, training=True)\n",
    "        loss = distillation_loss(y, student_logits, teacher_logits)\n",
    "    \n",
    "    gradients = tape.gradient(loss, distilled_student_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, distilled_student_model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Distillation training loop\n",
    "optimizer = optimizers.Adam(learning_rate=0.00035361)\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        batch_loss = train_step(x_batch, y_batch)\n",
    "        train_loss += batch_loss\n",
    "        num_batches += 1\n",
    "        if num_batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    num_batches = 0\n",
    "    for i in range(0, len(x_test), batch_size):\n",
    "        x_val_batch = x_test[i:i+batch_size]\n",
    "        y_val_batch = y_test[i:i+batch_size]\n",
    "        y_val_batch = tf.cast(y_val_batch, tf.int64)  # Cast y_val_batch to int64\n",
    "        val_logits = distilled_student_model(x_val_batch, training=False)\n",
    "        val_loss += distillation_loss(y_val_batch, val_logits, best_teacher_model(x_val_batch, training=False))\n",
    "        val_accuracy += tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_logits, axis=1), tf.squeeze(y_val_batch)), tf.float32))\n",
    "        num_batches += 1\n",
    "    \n",
    "    val_loss /= num_batches\n",
    "    val_accuracy /= num_batches\n",
    "    print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Save the distilled student model\n",
    "distilled_student_model.save('distilled_student_model_final.h5.keras')\n",
    "\n",
    "# Evaluate all models\n",
    "def evaluate_model(model, name):\n",
    "    eval_results = model.evaluate(x_test, y_test)\n",
    "    print(f'{name} - Test loss: {eval_results[0]:.4f}, Test accuracy: {eval_results[1]:.4f}')\n",
    "\n",
    "evaluate_model(tf.keras.models.load_model('teacher_model_best.h5.keras'), 'Teacher Model')\n",
    "evaluate_model(tf.keras.models.load_model('student_model_best.h5.keras'), 'Student Model')\n",
    "evaluate_model(distilled_student_model, 'Distilled Student Model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
